{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lab 14 Homework By Aidan Hennessy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 1 Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the six chunks of code immediately below, I upload some of the necessary libraries, \n",
    "### uploaded the necessary file, filtered the number of rows to 20,000, dropped na values and \n",
    "### preprocessed the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33822bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333945a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file = pd.read_csv('Reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file_red = ah_lab14_file[:20000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file_red = ah_lab14_file_red.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7278199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab14_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "       text = re.sub(r'http\\S+', '', text)\n",
    "       text = re.sub(r'@\\S+', '', text)\n",
    "       text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "       text = text.lower()\n",
    "       text = ' '.join([word for word in \n",
    "                        text.split() if word not in Lab14_stop_words])\n",
    "       return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the three chunks of code immediately below, I create a file with two new columns,\n",
    "### one with clean texts and one with tokenized texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a70864",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file_red_two = ah_lab14_file_red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdbde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file_red_two['cleaned_text'] = ah_lab14_file_red_two['Text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_file_red_two['tokenized_column'] = ah_lab14_file_red_two['cleaned_text'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I import two more necessary libraries, then I filter the list of vocabulary words\n",
    "### to just the ones that appear more than once and in fewer than 80 percent of the reviews. \n",
    "### Then I create two dataframes, \"cleaned_articles\" and \"tokenized_articles\"\n",
    "### that will be useful in the next parts of this lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42542c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_e2_file = ah_lab14_file_red_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(data, min_freq, max_prop):\n",
    "\n",
    "    ah_lab14_P2_words = []\n",
    "    for text in data:\n",
    "        ah_lab14_P2_tokens = word_tokenize(text.lower())  \n",
    "        ah_lab14_P2_words.extend(ah_lab14_P2_tokens)\n",
    "    \n",
    "    word_counts = Counter(ah_lab14_P2_words)\n",
    "\n",
    "    filtered_vocab = {}\n",
    "    for word, count in word_counts.items():\n",
    "        proportion = count / len(data) \n",
    "        if count >= min_freq and proportion <= max_prop:\n",
    "            filtered_vocab[word] = count\n",
    "\n",
    "    return filtered_vocab\n",
    "\n",
    "vocab_data = ah_lab14_e2_file['cleaned_text']\n",
    "\n",
    "\n",
    "vocabulary = create_vocabulary(vocab_data, 2, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = ah_lab14_file_red_two['cleaned_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_articles = ah_lab14_file_red_two['tokenized_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "### There is no Exercise 1 Part 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d00943",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 1 Part 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I vectorize the data then fit transform it twice. Then, I transform the data into tfidf data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_vec = CountVectorizer(analyzer='word')\n",
    "ah_lab14_tdm = ah_lab14_vec.fit_transform(cleaned_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = ah_lab14_vec.fit_transform([\" \".join(text) for text in tokenized_articles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_tfidf_transform = TfidfTransformer()\n",
    "ah_lab14_tfidf_mat = ah_lab14_tfidf_transform.fit_transform(doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I import the NMF library and then I create five components aka five topics for \n",
    "### my first Non-Negative Matrix Factorization (NMF) model. I then fit transform the\n",
    "### data before using the  \"get_feature_names_out().\" Then, I identify the 10 most \n",
    "### frequently appearing words in each of the five discovered NMF topics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295106a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics_five = 5\n",
    "ah_lab14_nmf_five = NMF(n_components=num_of_topics_five)\n",
    "ah_lab14_tfidf_mat_nmf_five = ah_lab14_nmf_five.fit_transform(ah_lab14_tfidf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_terms = ah_lab14_vec.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b211c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(ah_lab14_nmf_five.components_):\n",
    "    top_words_per_topic_idx = topic.argsort()[:-11:-1]\n",
    "    top_words_per_topic = [ah_lab14_terms[idx] for idx in top_words_per_topic_idx]\n",
    "    print(f\"NMF Topic {i + 1}: {', '.join(top_words_per_topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb88d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF Topic 1: great, product, good, love, like, chocolate, price, chips, taste, buy\n",
    "NMF Topic 2: coffee, cup, strong, blend, bold, flavor, roast, keurig, kcups, like\n",
    "NMF Topic 3: tea, green, teas, grey, loose, earl, iced, bags, love, flavor\n",
    "NMF Topic 4: dog, treats, food, dogs, treat, loves, br, eat, one, cat\n",
    "NMF Topic 5: juice, br, drink, sugar, soda, like, orange, taste, switch, sweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49de77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_lda_five = LatentDirichletAllocation(n_components=num_of_topics_five)\n",
    "ah_lab14_tfidf_mat_lda_five = ah_lab14_lda_five.fit_transform(ah_lab14_tfidf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(ah_lab14_lda_five.components_):\n",
    "    top_words_per_topic_idx = topic.argsort()[:-11:-1]\n",
    "    top_words_per_topic = [ah_lab14_terms[idx] for idx in top_words_per_topic_idx]\n",
    "    print(f\"LDA Topic {i + 1}: {', '.join(top_words_per_topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA Topic 1: clam, chowder, cavenders, span, minsbr, classtiny, jumbo, cotton, length, ibs\n",
    "LDA Topic 2: juice, chips, drink, soda, like, taste, flavor, peanut, br, butter\n",
    "LDA Topic 3: lobster, pocky, cheetos, jerk, rainbow, ploy, camper, dulce, leche, bisque\n",
    "LDA Topic 4: great, product, good, like, br, food, one, love, would, taste\n",
    "LDA Topic 5: coffee, tea, cup, flavor, like, taste, good, strong, one, blend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a153bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In Part 3 and Part 4, I notice topics that are largely focused on food and drink. \n",
    "### The difference between the two is that the NMF model has one topic about cats and dogs\n",
    "### while the LDA model has zero topics about cats and dogs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exericsse 1 Part 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I repeat everything I did in Part 3 and Part 4 in the four chunks of code immediately below\n",
    "### for Part 5, but instead of their being five topics for the second LDA model and the second \n",
    "### NMF model, there are nine topics for the second LDA model and the second NMF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd16e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics_nine = 9\n",
    "ah_lab14_nmf_nine = NMF(n_components=num_of_topics_nine)\n",
    "ah_lab14_tfidf_mat_nine = ah_lab14_nmf_nine.fit_transform(ah_lab14_tfidf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(ah_lab14_nmf_nine.components_):\n",
    "    top_words_per_topic_idx = topic.argsort()[:-11:-1]\n",
    "    top_words_per_topic = [ah_lab14_terms[idx] for idx in top_words_per_topic_idx]\n",
    "    print(f\"NMF Topic {i + 1}: {', '.join(top_words_per_topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286920e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF Topic 1: like, br, good, one, taste, really, dont, tried, would, ive\n",
    "NMF Topic 2: coffee, cup, strong, bold, blend, flavor, roast, smooth, bitter, keurig\n",
    "NMF Topic 3: tea, green, teas, grey, loose, earl, iced, bags, flavor, love\n",
    "NMF Topic 4: dog, treats, food, dogs, treat, loves, love, eat, cat, newmans\n",
    "NMF Topic 5: juice, drink, soda, sugar, orange, switch, fruit, br, carbonated, sweet\n",
    "NMF Topic 6: great, product, price, amazon, find, love, store, buy, order, local\n",
    "NMF Topic 7: chips, salt, potato, flavor, bag, kettle, love, vinegar, chip, bags\n",
    "NMF Topic 8: peanut, butter, pb, fat, calories, love, tastes, regular, protein, great\n",
    "NMF Topic 9: chocolate, hot, cocoa, milk, dark, best, cup, love, flavor, keurig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ba2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_lda_nine = LatentDirichletAllocation(n_components=num_of_topics_nine)\n",
    "ah_lab14_tfidf_mat_lda_nine = ah_lab14_lda_nine.fit_transform(ah_lab14_tfidf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(ah_lab14_lda_nine.components_):\n",
    "    top_words_per_topic_idx = topic.argsort()[:-11:-1]\n",
    "    top_words_per_topic = [ah_lab14_terms[idx] for idx in top_words_per_topic_idx]\n",
    "    print(f\"LDA Topic {i + 1}: {', '.join(top_words_per_topic)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e72b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA Topic 1: sausages, bahlsen, earths, hairball, stong, brittle, triplets, bent, unappealing, tulleys\n",
    "LDA Topic 2: olives, baronet, definatly, tourangelle, marzipan, reccommend, sprinkles, martinis, turkish, daves\n",
    "LDA Topic 3: coffee, tea, flavor, like, taste, good, cup, drink, great, one\n",
    "LDA Topic 4: mic, icicle, soil, gnats, usb, recording, potting, tast, rainbow, audio\n",
    "LDA Topic 5: waffle, stonewall, waffles, loaf, costa, ants, belgian, span, classtiny, minsbr\n",
    "LDA Topic 6: altoids, grinds, gunpowder, roastbr, sumatran, revolution, dulce, smashed, pear, leche\n",
    "LDA Topic 7: popcorn, sprinkles, carrot, kernels, chips, virginia, jimmies, popper, skippy, oak\n",
    "LDA Topic 8: great, like, good, product, br, food, love, one, taste, dog\n",
    "LDA Topic 9: trays, beaba, merrick, dha, freezing, gelatin, coriander, earths, kracker, chest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca908c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the LDA model and NMF model of Part 5, the NMF model's topics focus very strongly on food and drink. \n",
    "### But the LDA model's topics foucs a lot less on food and drink than the NMF model's topics do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ee00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 2 Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cc986",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I import the final libraries we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Nmf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac00af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I upload a dictionary and a corpus to be used for our models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_articles)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cba9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This creates the Gensim version of an LDA model for when there are five topics\n",
    "### and a Gensim version of an NMF model when there are five topics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_Lda_five = LdaModel(corpus, num_topics=num_of_topics_five, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_Nmf_five_rev = Nmf(corpus, num_topics=num_of_topics_five, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The two chunks of code immediately below calculate the first Sklearn LDA's logarithmic perplexity score\n",
    "### and the first Sklearn NMF's RMSE score. They are not directly comparable but they are comparable\n",
    "### enough that it is better to use an LDA model with a negative logarithmic perplexity score than \n",
    "### an NMF with an rmse of 0.005. While it would have been better to have used the NMF model if the \n",
    "### LDA model's perplexity score was greater than the NMF rmse of 0.005. But since this is not the case, the\n",
    "### results here favors the LDA model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98118f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_perplexity_lda = ah_lab14_Lda_five.log_perplexity(corpus)\n",
    "\n",
    "print(f\"First LDA Logarithmic Perplexity Score: {log_perplexity_lda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "First LDA Logarithmic Perplexity Score: -8.055354816573612\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82991a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_five = ah_lab14_nmf_five.transform(tfidf_matrix) \n",
    "H_five = ah_lab14_nmf_five.components_ \n",
    "\n",
    "mod_reconstructed_five = np.dot(W_five, H_five)\n",
    "\n",
    "rmse_five = np.sqrt(mean_squared_error(tfidf_matrix.toarray(), mod_reconstructed_five))\n",
    "\n",
    "print(f\"First RMSE: {rmse_five}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "First RMSE: 0.0052836857706265354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a69f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the five chunks of code immediately below, I add the coherence model library.\n",
    "### Then I calculate the tbe first Gensim LDA's CV Coherence score and UCI Coherence Score \n",
    "### as well as the first Gensim NMF's CV Coherence score and UCI Coherence socre. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b69d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27027ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_cv = CoherenceModel(model=ah_lab14_Lda_five, texts=tokenized_articles, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_lda_cv = coherence_model_lda_cv.get_coherence()\n",
    "\n",
    "print(f\"LDA CV Coherence Score: {coherence_score_lda_cv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ffbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA CV Coherence Score: 0.3799830888124176\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_cv = CoherenceModel(model=ah_lab14_Nmf_five_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_nmf_cv = coherence_model_nmf_cv.get_coherence()\n",
    "\n",
    "print(f\"NMF CV Coherence Score: {coherence_score_nmf_cv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF CV Coherence Score: 0.41088751301450044\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad637a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_uci = CoherenceModel(model=ah_lab14_Lda_five, texts=tokenized_articles, dictionary=dictionary, coherence='c_uci')\n",
    "coherence_score_lda_uci = coherence_model_lda_uci.get_coherence()\n",
    "\n",
    "print(f\"LDA UCI Coherence Score: {coherence_score_lda_uci}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA UCI Coherence Score: -0.09891672119933814\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9dccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_uci = CoherenceModel(model=ah_lab14_Nmf_five_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_uci')\n",
    "coherence_score_nmf_uci = coherence_model_nmf_uci.get_coherence()\n",
    "\n",
    "print(f\"NMF UCI Coherence Score: {coherence_score_nmf_uci}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF UCI Coherence Score: -0.030902851346895827\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the four chunks of code immediately below, I calculate the tbe first Gensim LDA's \n",
    "### NPMI Coherence score and UMass Coherence Score as well as the first Gensim NMF's \n",
    "### NPMI Coherence score and UMass Coherence socre. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68146bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_npmi = CoherenceModel(model=ah_lab14_Lda_five, texts=tokenized_articles, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score_lda_npmi = coherence_model_lda_npmi.get_coherence()\n",
    "\n",
    "print(f\"LDA NPMI Coherence Score: {coherence_score_lda_npmi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575743df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA NPMI Coherence Score: 0.0012342307914701741\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_npmi = CoherenceModel(model=ah_lab14_Nmf_five_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score_nmf_npmi = coherence_model_nmf_npmi.get_coherence()\n",
    "\n",
    "print(f\"NMF NPMI Coherence Score: {coherence_score_nmf_npmi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF NPMI Coherence Score: 0.007334133063895502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3650ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_umass = CoherenceModel(model=ah_lab14_Lda_five, texts=tokenized_articles, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_score_lda_umass = coherence_model_lda_umass.get_coherence()\n",
    "\n",
    "print(f\"LDA UMass Coherence Score: {coherence_score_lda_umass}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA UMass Coherence Score: -2.011368268767135\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_umass = CoherenceModel(model=ah_lab14_Nmf_five_rev, texts=tokenized_articles, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_score_nmf_umass = coherence_model_nmf_umass.get_coherence()\n",
    "\n",
    "print(f\"NMF UMass Coherence Score: {coherence_score_nmf_umass}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF UMass Coherence Score: -2.1324941058369515\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d174fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The first Gensim NMF's CV Coherenece score is higher than the first Gensim's \n",
    "### LDA CV Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the first Gensim NMF. \n",
    "\n",
    "### The first Gensim NMF's UCI Coherenece score is higher than the first Gensim's \n",
    "### LDA UCI Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the first Gensim NMF.\n",
    "\n",
    "### The first Gensim NMF's NPMI Coherenece score is higher than the first Gensim's \n",
    "### LDA NPMI Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the first Gensim NMF. \n",
    "\n",
    "### The first Gensim LDA's Umass Coherenece score is higher than the first Gensim's \n",
    "### NMF UMass Coherence score. The lower score is better, so these results work \n",
    "### in the favor of the first Gensim NMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62feea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the two chunks of code immediately below, I calculate the Silhouette score for the first Sklearn \n",
    "### LDA model and the second Sklearn NMF model. The result was that the first Sklearn LDA model has a \n",
    "### higher Silhouette score than the first Sklearn NMF model, a result favorable for the first Sklearn \n",
    "### LDA model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg_lda = silhouette_score(ah_lab14_tfidf_mat_lda_five, np.argmax(ah_lab14_tfidf_mat_lda_five, axis=1)) \n",
    "print(f\"First LDA Silhouette Score: {silhouette_avg_lda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "First LDA Silhouette Score: 0.7034540099531483\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9157f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg_nmf = silhouette_score(ah_lab14_tfidf_mat_nmf_five, np.argmax(ah_lab14_tfidf_mat_nmf_five, axis=1)) # Get the cluster assignment for each data point and use it to calculate Silhouette score\n",
    "print(f\"First NMF Silhouette Score: {silhouette_avg_nmf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "First NMF Silhouette Score: 0.45043858421447386\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7434da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the LDA and NMF models with five topics, the perplexity score-rmse comparison favors LDA,\n",
    "### and that comes from sklearn. All four Coherence model scores favor NMF, and they are all in gensim. \n",
    "### And the Silhouette score comparison favors LDA when compared to NMF, and the Silhouette scores \n",
    "### both come from sklearn. Both sklearn results favor LDA but the gensim results favor NMF. There \n",
    "### are four gensim results and two sklearn results, so in this case, the NMF model is preferred. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36387ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the two chunks of code immediately below, we create the second Gensim LDA model \n",
    "### and the second Gensim NMF model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a070722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_Lda_nine = LdaModel(corpus, num_topics=num_of_topics_nine, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "ah_lab14_Nmf_nine_rev = Nmf(corpus, num_topics=num_of_topics_nine, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317900d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This calculates the Sklearn LDA's logarithmic perplexity score and the Sklearn NMF's RMSE score. \n",
    "### The LDA model has a negative perplexity score while rmse is calculated to equal 0.005. While \n",
    "### these metrics are not entirely comparable, they are comparable enough that it is better to use\n",
    "### an LDA model with a negative logarithmic perplexity score than an NMF model with an rmse of 0.005. \n",
    "### If the logarithmic perplexity score of the LDA model was greater than 0.005 and the NMF model's rmse stayed at 0.005,\n",
    "### we would have gone with the NMF model. But the LDA model has a logarithmic property that is below 0.005, \n",
    "### so the LDA model wins here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbbd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_perplexity_lda_nine = ah_lab14_Lda_nine.log_perplexity(corpus)\n",
    "\n",
    "print(f\"Second LDA Logarithmic Perplexity Score: {log_perplexity_lda_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second LDA Logarithmic Perplexity Score: -8.442564232470644\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37756534",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_nine = ah_lab14_nmf_nine.transform(tfidf_matrix) \n",
    "H_nine = ah_lab14_nmf_nine.components_ \n",
    "\n",
    "mod_reconstructed_nine = np.dot(W_nine, H_nine)\n",
    "\n",
    "rmse_nine = np.sqrt(mean_squared_error(tfidf_matrix.toarray(), mod_reconstructed_nine))\n",
    "\n",
    "print(f\"Second RMSE: {rmse_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second RMSE: 0.005270011251954697\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b178f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the four chunks of code immediately below, I calculate the tbe second Gensim LDA's \n",
    "### CV Coherence score and UCI Coherence Score as well as the second Gensim NMF's CV Coherence \n",
    "### score and UCI Coherence score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92db257",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_cv_nine = CoherenceModel(model=ah_lab14_Lda_nine, texts=tokenized_articles, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_lda_cv_nine = coherence_model_lda_cv_nine.get_coherence()\n",
    "\n",
    "print(f\"LDA CV Coherence Score Nine: {coherence_score_lda_cv_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ceae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA CV Coherence Score Nine: 0.39641133795694006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025aafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_cv_nine = CoherenceModel(model=ah_lab14_Nmf_nine_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score_nmf_cv_nine = coherence_model_nmf_cv_nine.get_coherence()\n",
    "\n",
    "print(f\"NMF CV Coherence Score Nine: {coherence_score_nmf_cv_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70798b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF CV Coherence Score Nine: 0.4640577058992772\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b192514",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_uci_nine = CoherenceModel(model=ah_lab14_Lda_nine, texts=tokenized_articles, dictionary=dictionary, coherence='c_uci')\n",
    "coherence_score_lda_uci_nine = coherence_model_lda_uci_nine.get_coherence()\n",
    "\n",
    "print(f\"LDA UCI Coherence Score Nine: {coherence_score_lda_uci_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aea74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA UCI Coherence Score Nine: -1.321361157016826\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_uci_nine = CoherenceModel(model=ah_lab14_Nmf_nine_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_uci')\n",
    "coherence_score_nmf_uci_nine = coherence_model_nmf_uci_nine.get_coherence()\n",
    "\n",
    "print(f\"NMF UCI Coherence Score Nine: {coherence_score_nmf_uci_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86eaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF UCI Coherence Score Nine: 0.04269615535667522\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b21bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the four chunks of code immediately below, I add the coherence model library.\n",
    "### Then I calculate the tbe second Gensim LDA's NPMI Coherence score and UMass Coherence Score \n",
    "### as well as the second Gensim NMF's NPMI Coherence score and UMass Coherence score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_npmi_nine = CoherenceModel(model=ah_lab14_Lda_nine, texts=tokenized_articles, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score_lda_npmi_nine = coherence_model_lda_npmi_nine.get_coherence()\n",
    "\n",
    "print(f\"LDA NPMI Coherence Score Nine: {coherence_score_lda_npmi_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA NPMI Coherence Score Nine: -0.034141667205064376\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_npmi_nine = CoherenceModel(model=ah_lab14_Nmf_nine_rev, texts=tokenized_articles, dictionary=dictionary, coherence='c_npmi')\n",
    "coherence_score_nmf_npmi_nine = coherence_model_nmf_npmi_nine.get_coherence()\n",
    "\n",
    "print(f\"NMF NPMI Coherence Score Nine: {coherence_score_nmf_npmi_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfe809",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF NPMI Coherence Score Nine: 0.022497966553568616\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9934761",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_umass_nine = CoherenceModel(model=ah_lab14_Lda_nine, texts=tokenized_articles, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_score_lda_umass_nine = coherence_model_lda_umass_nine.get_coherence()\n",
    "\n",
    "print(f\"LDA UMass Coherence Score Nine: {coherence_score_lda_umass_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7de9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA UMass Coherence Score Nine: -3.487895572670281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf_umass_nine = CoherenceModel(model=ah_lab14_Nmf_nine_rev, texts=tokenized_articles, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_score_nmf_umass_nine = coherence_model_nmf_umass_nine.get_coherence()\n",
    "\n",
    "print(f\"NMF UMass Coherence Score Nine: {coherence_score_nmf_umass_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05971ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF UMass Coherence Score Nine: -2.2915215454603572\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead34afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The second Gensim NMF's CV Coherenece score is higher than the second Gensim's \n",
    "### LDA CV Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the second Gensim NMF. \n",
    "\n",
    "### The second Gensim NMF's UCI Coherenece score is higher than the second Gensim's \n",
    "### LDA UCI Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the second Gensim NMF.\n",
    "\n",
    "### The second Gensim NMF's NPMI Coherenece score is higher than the second Gensim's \n",
    "### LDA NPMI Coherence score. The higher score is better, so these results work \n",
    "### in the favor of the first Gensim NMF. \n",
    "\n",
    "### The second Gensim LDA's Umass Coherenece score is higher than the second Gensim's \n",
    "### UMass Coherence score. The lower score is better, so these results work \n",
    "### in the favor of the first Gensim LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the two chunks of code immediately below, I calculate the Silhouette score for the second Sklearn \n",
    "### LDA model and the second Sklearn NMF model. The result was that the second Sklearn LDA model has a \n",
    "### higher Silhouette score than the second Sklearn NMF model, a result favorable for the second Sklearn \n",
    "### LDA model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dcfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg_lda_nine = silhouette_score(ah_lab14_tfidf_mat_lda_nine, np.argmax(ah_lab14_tfidf_mat_lda_nine, axis=1)) \n",
    "print(f\"Second LDA Silhouette Score: {silhouette_avg_lda_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second LDA Silhouette Score: 0.6676928757234112\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg_nmf_nine = silhouette_score(ah_lab14_tfidf_mat_nine, np.argmax(ah_lab14_tfidf_mat_nine, axis=1)) # Get the cluster assignment for each data point and use it to calculate Silhouette score\n",
    "print(f\"Second NMF Silhouette Score: {silhouette_avg_nmf_nine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second NMF Silhouette Score: 0.24972796948024104\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1872ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the LDA model and the NMF model where there are nine topics, the LDA's logarithmic perplexity score\n",
    "### and the NMF's rmse that are both from sklearn have results that favor the LDA model.Three out of the four coherence\n",
    "### scores from gensim favor the NMF model and one favors the LDA model. The Silhouette scores of these two models\n",
    "### favor the LDA model. Both sklearn results favor LDA but three out of four gensim results favor NMF. There \n",
    "### are four gensim results and two sklearn results, so in this case, the LDA model and the NMF model tie in terms\n",
    "### of preferred model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9dc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Please check out corresponding CSV file for more info on comparisons. \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
